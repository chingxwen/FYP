{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from pprint import pprint\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "import wordcloud\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading of .csv file\n",
    "df = pd.read_csv(r\"C:\\Users\\Jurgen\\Desktop\\FYP\\News\\Misc\\TheGuardian.csv\")\n",
    "content = df['Text']\n",
    "assert content.isnull().count() == len(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4193, 8625, 9258, 2648, 13182, 1899, 7842, 11971, 8772, 5565, 9165, 3198, 2426, 8657, 9182, 12405, 11333, 25352, 7375, 12506, 12845, 3788, 23099, 7351, 7728, 4338, 10207, 11862, 10192, 10773, 5184, 13471, 1028, 6678, 3873, 4470, 10071, 7516, 2964, 3423, 6897, 7321, 6019, 2920, 1614, 1004, 3680, 8942, 6232, 8002, 16608, 7648, 7448, 8712, 8582, 7375, 4079, 8308, 2319, 1882, 7517, 5742, 6646, 638, 7839, 7870, 624, 20102, 10232, 5633, 4097, 3218, 7325, 6543, 3348, 7306, 4280, 9235, 9615, 3016, 1582, 1119, 4117, 6499, 4670, 3604, 10180, 3544, 5768, 9769, 6767, 3352, 828, 5165, 4172, 1195, 1460, 3413, 2723, 7797, 9302, 12164, 5348, 5273, 7782, 3729, 3376, 3347, 4418, 7799, 1798, 6489, 2310, 3982, 9678, 7603, 11839, 720, 5915, 3332, 7701, 5279, 9187, 13448, 5879, 5599, 8929, 2212, 1950, 26567, 3209, 21282, 5188, 4600, 8239, 1318, 4899, 4707, 4045, 7950, 6905, 5694, 4248, 6604, 13625, 5775, 6385, 6005, 7459, 9072, 16628, 8515, 7515, 2805, 11277, 4662, 3423, 9310, 6415, 9101, 7395, 10163, 3749, 6962, 6004, 2551, 7063, 3189, 7154, 9514, 3657, 8356, 9881, 12059, 7653, 2324, 5158, 7010, 8721, 6863, 7818, 31668, 8641, 8062, 1877, 3927, 5917, 3298, 4480, 6634, 4576, 10661, 1337, 2040, 3964, 5096, 270, 4200, 14125, 1955]\n"
     ]
    }
   ],
   "source": [
    "#Before stop word removal count\n",
    "count = [len(content[i]) for i in range(len(content))]\n",
    "# data['Count Before'] = count\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [Google, has, snapped, up, the, Fitbit, activi...\n",
       "1     [Brilliant, camera, slick, features, and, smal...\n",
       "2     [On, a, dark, dark, night, in, a, dark, dark, ...\n",
       "3     [Labour, has, written, to, the, competition, r...\n",
       "4     [Google, s, latest, Pixel, 4, XL, smartphone, ...\n",
       "5     [Apple, has, launched, a, new, set, of, its, p...\n",
       "6     [I, use, a, 9, 7in, Samsung, Galaxy, Tab, S2, ...\n",
       "7     [Always, on, screen, completes, the, package, ...\n",
       "8     [Still, an, absolute, beast, in, every, way, e...\n",
       "9     [First, it, was, books, then, it, was, the, Ki...\n",
       "10    [The, OnePlus, 7T, takes, the, best, bits, of,...\n",
       "11    [In, a, country, where, the, biggest, companie...\n",
       "12    [I, think, you, should, at, least, make, an, e...\n",
       "13    [Plastic, packaging, has, many, amazing, quali...\n",
       "14    [A, great, camera, screen, and, performance, c...\n",
       "15    [After, a, period, of, stagnation, the, A, Lea...\n",
       "16    [Apple, s, iPhone, has, gone, pro, for, its, 1...\n",
       "17    [Gordon, s, wine, bar, is, reached, through, a...\n",
       "18    [Microsoft, has, launched, the, Surface, Pro, ...\n",
       "19    [Apple, s, lower, cost, model, has, latest, ch...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenization\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "data = []\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "content = content.apply(lambda x: tokenizer.tokenize(x))\n",
    "content.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removal of Stop Words\n",
    "from nltk.corpus import stopwords\n",
    "set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = [w for w in text if w not in stopwords.words('english')]\n",
    "    return words\n",
    "\n",
    "content = content.apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def word_lemmatizer(text):\n",
    "    lem_text = [lemmatizer.lemmatize(i, pos=\"a\") for i in text]\n",
    "    lem_text = [lemmatizer.lemmatize(i, pos=\"s\") for i in text]\n",
    "    lem_text = [lemmatizer.lemmatize(i, pos=\"v\") for i in text]\n",
    "    lem_text = [lemmatizer.lemmatize(i, pos=\"r\") for i in text]\n",
    "    lem_text = [lemmatizer.lemmatize(i, pos=\"n\") for i in text]\n",
    "    return lem_text\n",
    "\n",
    "content = content.apply(lambda x: word_lemmatizer(x))\n",
    "print(content[0])\n",
    "content_freq = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count after removal of stop words\n",
    "countaft = [len(content[i]) for i in range(len(content))]\n",
    "print(countaft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tagging\n",
    "content_tag = [nltk.pos_tag(i) for i in content]\n",
    "print(content_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for articles in content_tag:\n",
    "#     for words in articles:\n",
    "#         if words[1] == 'NNP':\n",
    "#             print(words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Difference for the count before and after stop word removal\n",
    "count_diff = []\n",
    "for i in range(len(count)):\n",
    "    count_diff.append(round(((count[i] - countaft[i])/count[i])*100, 1))\n",
    "print(count_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Freq = [FreqDist(content_freq[i]).most_common(5) for i in range(len(content_freq))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Content with Tag'] = content_tag\n",
    "df['Amount Cleansed (%)'] = count_diff\n",
    "df['Frequency'] = Freq\n",
    "\n",
    "# df.to_csv('Output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feeling_words = []\n",
    "word_total_count = 0\n",
    "for articles in content_tag:\n",
    "    word_total_count += len(articles)\n",
    "    for words in articles:\n",
    "        if words[1].startswith('JJ'):\n",
    "            feeling_words.append((words))\n",
    "\n",
    "print(feeling_words)\n",
    "         \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_fw = []\n",
    "\n",
    "for articles in content_tag:\n",
    "    article_fw = []\n",
    "    for words in articles:\n",
    "        if words[1].startswith('JJ'):\n",
    "            article_fw.append(words)\n",
    "    total_fw.append(article_fw)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    \"\"\"\n",
    "    Convert between the PennTreebank tags to simple Wordnet tags\n",
    "    \"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(word,tag):\n",
    "    \"\"\" returns list of pos neg and objective score. But returns empty list if not present in senti wordnet. \"\"\"\n",
    "\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
    "        return []\n",
    "\n",
    "    lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "    if not lemma:\n",
    "        return []\n",
    "\n",
    "    synsets = wn.synsets(word, pos=wn_tag)\n",
    "    if not synsets:\n",
    "        return []\n",
    "\n",
    "    # Take the first sense, the most common\n",
    "    synset = synsets[0]\n",
    "    swn_synset = swn.senti_synset(synset.name())\n",
    "\n",
    "    return [swn_synset.pos_score(),swn_synset.neg_score(),swn_synset.obj_score()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in total_fw:\n",
    "    for words in article:\n",
    "        penn_to_wn(words[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_list = []\n",
    "# neg_list = []\n",
    "net_senti = []\n",
    "for article in total_fw:\n",
    "#     article_pos = []\n",
    "#     article_neg = []\n",
    "    article_senti = []\n",
    "    for words in article:\n",
    "        \n",
    "        senti = get_sentiment(words[0], words[1])\n",
    "        if len(senti) > 0:\n",
    "            print([words[0], (senti[0] - senti[1])])\n",
    "            article_senti.append([words[0], (senti[0] - senti[1])])\n",
    "    net_senti.append(article_senti)\n",
    "            \n",
    "#             if senti[0]>senti[1]:\n",
    "#                 print(\"pos: \" + words[0] + str(senti))\n",
    "# #                 article_pos.append([words[0],senti[0]])\n",
    "#             elif senti[1]>senti[0]:\n",
    "#                 print(\"neg: \" + words[0] + str(senti))\n",
    "# #                 article_neg.append([words[0],senti[1]])\n",
    "#     pos_list.append(article_pos)\n",
    "#     neg_list.append(article_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for values in net_senti[0]:\n",
    "    print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_senti = []\n",
    "for article in net_senti:\n",
    "    sentiment = 0\n",
    "    article_word_count = 0\n",
    "    for senti in article:\n",
    "        print(senti[0], senti[1])\n",
    "        sentiment += senti[1]\n",
    "        article_word_count += 1\n",
    "    print(\"\\n********************************************\\nTotal Sentiment: \", sentiment)\n",
    "    print(\"Word Count In Article: \", article_word_count, \"\\n********************************************\")\n",
    "    print(\"\\n**********************\\n NEXT ARTICLE \\n**********************\")\n",
    "    article_senti.append(sentiment)\n",
    "\n",
    "# for indivsenti in article_senti:\n",
    "#     if senti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(article_senti)):\n",
    "        print(article_senti[i])\n",
    "print(len(article_senti))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(net_senti[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_art = 0\n",
    "neg_art = 0\n",
    "\n",
    "for senti in article_senti:\n",
    "    if senti > 0:\n",
    "        pos_art += 1\n",
    "    elif senti < 0:\n",
    "        neg_art += 1\n",
    "    else:\n",
    "        pass\n",
    "print(\"Out Of {} Articles, {} Are Positive , {} Are Negative And {} Is/Are Neutral\".format(len(article_senti), pos_art, neg_art, (len(article_senti) - pos_art - neg_art)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_senti = 0\n",
    "# neg_senti = 0\n",
    "\n",
    "# for words in pos_list:\n",
    "#     pos_senti += words[1][0]\n",
    "\n",
    "# for words in neg_list:\n",
    "#     neg_senti += words[1][0]\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
