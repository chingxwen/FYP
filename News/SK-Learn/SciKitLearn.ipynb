{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News_Senti</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Open_USD</th>\n",
       "      <th>High_USD</th>\n",
       "      <th>Low_USD</th>\n",
       "      <th>Close_USD</th>\n",
       "      <th>Adj_Close_USD</th>\n",
       "      <th>Twitter_Senti</th>\n",
       "      <th>Reddit_Comment</th>\n",
       "      <th>Reddit_SelfText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-0.013258</td>\n",
       "      <td>13954800.0</td>\n",
       "      <td>0.168756</td>\n",
       "      <td>0.168900</td>\n",
       "      <td>0.164937</td>\n",
       "      <td>0.165278</td>\n",
       "      <td>0.161698</td>\n",
       "      <td>0.354322</td>\n",
       "      <td>0.195632</td>\n",
       "      <td>0.065820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.070484</td>\n",
       "      <td>12513900.0</td>\n",
       "      <td>0.168580</td>\n",
       "      <td>0.169250</td>\n",
       "      <td>0.168421</td>\n",
       "      <td>0.168800</td>\n",
       "      <td>0.165144</td>\n",
       "      <td>0.354322</td>\n",
       "      <td>0.263295</td>\n",
       "      <td>0.223880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.033616</td>\n",
       "      <td>20286800.0</td>\n",
       "      <td>0.166439</td>\n",
       "      <td>0.168949</td>\n",
       "      <td>0.166439</td>\n",
       "      <td>0.168361</td>\n",
       "      <td>0.164714</td>\n",
       "      <td>0.354322</td>\n",
       "      <td>0.309500</td>\n",
       "      <td>-0.189340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.061636</td>\n",
       "      <td>67935700.0</td>\n",
       "      <td>0.178142</td>\n",
       "      <td>0.180494</td>\n",
       "      <td>0.176514</td>\n",
       "      <td>0.180262</td>\n",
       "      <td>0.176358</td>\n",
       "      <td>0.354322</td>\n",
       "      <td>0.316293</td>\n",
       "      <td>-0.402100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.120139</td>\n",
       "      <td>62404000.0</td>\n",
       "      <td>0.177505</td>\n",
       "      <td>0.184407</td>\n",
       "      <td>0.177301</td>\n",
       "      <td>0.183531</td>\n",
       "      <td>0.179556</td>\n",
       "      <td>0.354322</td>\n",
       "      <td>0.034783</td>\n",
       "      <td>0.420870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>-0.036621</td>\n",
       "      <td>25425400.0</td>\n",
       "      <td>0.184792</td>\n",
       "      <td>0.185902</td>\n",
       "      <td>0.183966</td>\n",
       "      <td>0.185742</td>\n",
       "      <td>0.181719</td>\n",
       "      <td>0.354322</td>\n",
       "      <td>0.073471</td>\n",
       "      <td>-0.102990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.092705</td>\n",
       "      <td>25587400.0</td>\n",
       "      <td>0.186901</td>\n",
       "      <td>0.187061</td>\n",
       "      <td>0.184615</td>\n",
       "      <td>0.184927</td>\n",
       "      <td>0.180922</td>\n",
       "      <td>0.354322</td>\n",
       "      <td>0.234032</td>\n",
       "      <td>0.261930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.101774</td>\n",
       "      <td>23492600.0</td>\n",
       "      <td>0.187136</td>\n",
       "      <td>0.187359</td>\n",
       "      <td>0.185055</td>\n",
       "      <td>0.186555</td>\n",
       "      <td>0.182515</td>\n",
       "      <td>-0.102700</td>\n",
       "      <td>0.199300</td>\n",
       "      <td>0.102714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.052211</td>\n",
       "      <td>24611200.0</td>\n",
       "      <td>0.183645</td>\n",
       "      <td>0.185186</td>\n",
       "      <td>0.183033</td>\n",
       "      <td>0.183795</td>\n",
       "      <td>0.180445</td>\n",
       "      <td>0.354322</td>\n",
       "      <td>0.228048</td>\n",
       "      <td>0.251273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.222372</td>\n",
       "      <td>27390100.0</td>\n",
       "      <td>0.204382</td>\n",
       "      <td>0.205071</td>\n",
       "      <td>0.202789</td>\n",
       "      <td>0.204337</td>\n",
       "      <td>0.200612</td>\n",
       "      <td>0.354322</td>\n",
       "      <td>0.138369</td>\n",
       "      <td>0.299167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.179139</td>\n",
       "      <td>33333000.0</td>\n",
       "      <td>0.204052</td>\n",
       "      <td>0.204658</td>\n",
       "      <td>0.200585</td>\n",
       "      <td>0.202163</td>\n",
       "      <td>0.198478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.123221</td>\n",
       "      <td>-0.052550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.310380</td>\n",
       "      <td>28654800.0</td>\n",
       "      <td>0.205414</td>\n",
       "      <td>0.208468</td>\n",
       "      <td>0.205173</td>\n",
       "      <td>0.207218</td>\n",
       "      <td>0.203440</td>\n",
       "      <td>0.354322</td>\n",
       "      <td>0.210979</td>\n",
       "      <td>0.533933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.199303</td>\n",
       "      <td>32042000.0</td>\n",
       "      <td>0.204459</td>\n",
       "      <td>0.205850</td>\n",
       "      <td>0.200871</td>\n",
       "      <td>0.201987</td>\n",
       "      <td>0.198305</td>\n",
       "      <td>0.354322</td>\n",
       "      <td>0.054587</td>\n",
       "      <td>0.273650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>-0.057383</td>\n",
       "      <td>33580500.0</td>\n",
       "      <td>0.202036</td>\n",
       "      <td>0.202434</td>\n",
       "      <td>0.195495</td>\n",
       "      <td>0.198783</td>\n",
       "      <td>0.195160</td>\n",
       "      <td>0.354322</td>\n",
       "      <td>0.179833</td>\n",
       "      <td>0.165186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.153396</td>\n",
       "      <td>29663900.0</td>\n",
       "      <td>0.195477</td>\n",
       "      <td>0.197756</td>\n",
       "      <td>0.193709</td>\n",
       "      <td>0.196849</td>\n",
       "      <td>0.193261</td>\n",
       "      <td>0.354322</td>\n",
       "      <td>0.383969</td>\n",
       "      <td>0.199500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.185523</td>\n",
       "      <td>26891000.0</td>\n",
       "      <td>0.196563</td>\n",
       "      <td>0.199753</td>\n",
       "      <td>0.195341</td>\n",
       "      <td>0.199402</td>\n",
       "      <td>0.195767</td>\n",
       "      <td>0.354322</td>\n",
       "      <td>0.062768</td>\n",
       "      <td>-0.250740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.181762</td>\n",
       "      <td>38358900.0</td>\n",
       "      <td>0.190121</td>\n",
       "      <td>0.193250</td>\n",
       "      <td>0.189893</td>\n",
       "      <td>0.191856</td>\n",
       "      <td>0.188359</td>\n",
       "      <td>0.354322</td>\n",
       "      <td>0.207924</td>\n",
       "      <td>-0.229400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.156127</td>\n",
       "      <td>91328700.0</td>\n",
       "      <td>0.187458</td>\n",
       "      <td>0.191126</td>\n",
       "      <td>0.183772</td>\n",
       "      <td>0.185606</td>\n",
       "      <td>0.182223</td>\n",
       "      <td>0.354322</td>\n",
       "      <td>0.334719</td>\n",
       "      <td>0.079950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.047075</td>\n",
       "      <td>25362600.0</td>\n",
       "      <td>0.188012</td>\n",
       "      <td>0.188138</td>\n",
       "      <td>0.185120</td>\n",
       "      <td>0.186678</td>\n",
       "      <td>0.183915</td>\n",
       "      <td>1.631300</td>\n",
       "      <td>0.130219</td>\n",
       "      <td>0.040545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>-0.023346</td>\n",
       "      <td>43098400.0</td>\n",
       "      <td>0.153150</td>\n",
       "      <td>0.155843</td>\n",
       "      <td>0.151955</td>\n",
       "      <td>0.155789</td>\n",
       "      <td>0.153483</td>\n",
       "      <td>0.354322</td>\n",
       "      <td>0.202148</td>\n",
       "      <td>0.054500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.270494</td>\n",
       "      <td>62026000.0</td>\n",
       "      <td>0.146427</td>\n",
       "      <td>0.150944</td>\n",
       "      <td>0.144945</td>\n",
       "      <td>0.150509</td>\n",
       "      <td>0.148281</td>\n",
       "      <td>0.197900</td>\n",
       "      <td>0.338811</td>\n",
       "      <td>0.625450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.210893</td>\n",
       "      <td>91312200.0</td>\n",
       "      <td>0.127730</td>\n",
       "      <td>0.129274</td>\n",
       "      <td>0.125974</td>\n",
       "      <td>0.126142</td>\n",
       "      <td>0.124275</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.252168</td>\n",
       "      <td>0.358890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.098321</td>\n",
       "      <td>41025300.0</td>\n",
       "      <td>0.132775</td>\n",
       "      <td>0.134781</td>\n",
       "      <td>0.131852</td>\n",
       "      <td>0.133832</td>\n",
       "      <td>0.131850</td>\n",
       "      <td>0.401900</td>\n",
       "      <td>0.019361</td>\n",
       "      <td>0.217208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.079191</td>\n",
       "      <td>27023200.0</td>\n",
       "      <td>0.136769</td>\n",
       "      <td>0.137502</td>\n",
       "      <td>0.135543</td>\n",
       "      <td>0.136241</td>\n",
       "      <td>0.134224</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.274506</td>\n",
       "      <td>-0.566967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.232194</td>\n",
       "      <td>32668100.0</td>\n",
       "      <td>0.149470</td>\n",
       "      <td>0.151278</td>\n",
       "      <td>0.148547</td>\n",
       "      <td>0.149076</td>\n",
       "      <td>0.146869</td>\n",
       "      <td>0.608100</td>\n",
       "      <td>0.221375</td>\n",
       "      <td>0.119625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.093342</td>\n",
       "      <td>25886200.0</td>\n",
       "      <td>0.154730</td>\n",
       "      <td>0.155503</td>\n",
       "      <td>0.153496</td>\n",
       "      <td>0.155343</td>\n",
       "      <td>0.153700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.147463</td>\n",
       "      <td>0.240143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.128877</td>\n",
       "      <td>19114300.0</td>\n",
       "      <td>0.171256</td>\n",
       "      <td>0.172645</td>\n",
       "      <td>0.169806</td>\n",
       "      <td>0.172047</td>\n",
       "      <td>0.170228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.272980</td>\n",
       "      <td>0.251909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    News_Senti      Volume  Open_USD  High_USD   Low_USD  Close_USD  \\\n",
       "0    -0.013258  13954800.0  0.168756  0.168900  0.164937   0.165278   \n",
       "1     0.070484  12513900.0  0.168580  0.169250  0.168421   0.168800   \n",
       "2     0.033616  20286800.0  0.166439  0.168949  0.166439   0.168361   \n",
       "3     0.061636  67935700.0  0.178142  0.180494  0.176514   0.180262   \n",
       "4     0.120139  62404000.0  0.177505  0.184407  0.177301   0.183531   \n",
       "5    -0.036621  25425400.0  0.184792  0.185902  0.183966   0.185742   \n",
       "6     0.092705  25587400.0  0.186901  0.187061  0.184615   0.184927   \n",
       "7     0.101774  23492600.0  0.187136  0.187359  0.185055   0.186555   \n",
       "8     0.052211  24611200.0  0.183645  0.185186  0.183033   0.183795   \n",
       "9     0.222372  27390100.0  0.204382  0.205071  0.202789   0.204337   \n",
       "10    0.179139  33333000.0  0.204052  0.204658  0.200585   0.202163   \n",
       "11    0.310380  28654800.0  0.205414  0.208468  0.205173   0.207218   \n",
       "12    0.199303  32042000.0  0.204459  0.205850  0.200871   0.201987   \n",
       "13   -0.057383  33580500.0  0.202036  0.202434  0.195495   0.198783   \n",
       "14    0.153396  29663900.0  0.195477  0.197756  0.193709   0.196849   \n",
       "15    0.185523  26891000.0  0.196563  0.199753  0.195341   0.199402   \n",
       "16    0.181762  38358900.0  0.190121  0.193250  0.189893   0.191856   \n",
       "17    0.156127  91328700.0  0.187458  0.191126  0.183772   0.185606   \n",
       "18    0.047075  25362600.0  0.188012  0.188138  0.185120   0.186678   \n",
       "19   -0.023346  43098400.0  0.153150  0.155843  0.151955   0.155789   \n",
       "20    0.270494  62026000.0  0.146427  0.150944  0.144945   0.150509   \n",
       "21    0.210893  91312200.0  0.127730  0.129274  0.125974   0.126142   \n",
       "22    0.098321  41025300.0  0.132775  0.134781  0.131852   0.133832   \n",
       "23    0.079191  27023200.0  0.136769  0.137502  0.135543   0.136241   \n",
       "24    0.232194  32668100.0  0.149470  0.151278  0.148547   0.149076   \n",
       "25    0.093342  25886200.0  0.154730  0.155503  0.153496   0.155343   \n",
       "26    0.128877  19114300.0  0.171256  0.172645  0.169806   0.172047   \n",
       "\n",
       "    Adj_Close_USD  Twitter_Senti  Reddit_Comment  Reddit_SelfText  \n",
       "0        0.161698       0.354322        0.195632         0.065820  \n",
       "1        0.165144       0.354322        0.263295         0.223880  \n",
       "2        0.164714       0.354322        0.309500        -0.189340  \n",
       "3        0.176358       0.354322        0.316293        -0.402100  \n",
       "4        0.179556       0.354322        0.034783         0.420870  \n",
       "5        0.181719       0.354322        0.073471        -0.102990  \n",
       "6        0.180922       0.354322        0.234032         0.261930  \n",
       "7        0.182515      -0.102700        0.199300         0.102714  \n",
       "8        0.180445       0.354322        0.228048         0.251273  \n",
       "9        0.200612       0.354322        0.138369         0.299167  \n",
       "10       0.198478       0.000000        0.123221        -0.052550  \n",
       "11       0.203440       0.354322        0.210979         0.533933  \n",
       "12       0.198305       0.354322        0.054587         0.273650  \n",
       "13       0.195160       0.354322        0.179833         0.165186  \n",
       "14       0.193261       0.354322        0.383969         0.199500  \n",
       "15       0.195767       0.354322        0.062768        -0.250740  \n",
       "16       0.188359       0.354322        0.207924        -0.229400  \n",
       "17       0.182223       0.354322        0.334719         0.079950  \n",
       "18       0.183915       1.631300        0.130219         0.040545  \n",
       "19       0.153483       0.354322        0.202148         0.054500  \n",
       "20       0.148281       0.197900        0.338811         0.625450  \n",
       "21       0.124275       0.000000        0.252168         0.358890  \n",
       "22       0.131850       0.401900        0.019361         0.217208  \n",
       "23       0.134224       0.340000        0.274506        -0.566967  \n",
       "24       0.146869       0.608100        0.221375         0.119625  \n",
       "25       0.153700       0.000000        0.147463         0.240143  \n",
       "26       0.170228       0.000000        0.272980         0.251909  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\Jurgen\\Desktop\\FYP\\Shared\\News_Twitter_Stock_Reddit_NaN.csv\")\n",
    "# df = df.drop(columns = [\"Reddit_SelfText\", \"Twitter_Senti\", \"News_Senti\"])\n",
    "# df = df.fillna(0)\n",
    "df = df.drop(['Date', 'Stockname', \"Stock_Difference\"], axis=1)\n",
    "# 'News_Senti', \"Twitter_Senti\", \"Reddit_Comment\", \"Reddit_SelfText\",\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "df.shape\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# X = df[[\"Reddit_Comment\"]].to_numpy()#.reshape(-1, 1)\n",
    "\n",
    "X = df.loc[:, df.columns != 'Adj_Close_USD'].to_numpy()\n",
    "# X = df.drop(['Stock_Difference', 'Date', 'Stockname', 'News_Senti', \"Twitter_Senti\", \"Reddit_Comment\", \"Reddit_SelfText\"], axis=1).to_numpy()\n",
    "\n",
    "y = df[\"Adj_Close_USD\"].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.75, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      " [ 1.73536405e-01 -3.81867849e-10  1.64414613e-01  1.63279066e-01\n",
      "  1.58760392e-01  1.57979728e-01  7.77876874e-02  1.17556101e-01\n",
      " -9.61995114e-04]\n",
      "Mean squared error: 0.00\n",
      "Coefficient of determination: -1.85\n",
      "Intercept:  0.01121197886938094\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "regr = LinearRegression()\n",
    "regr.fit(X_train, y_train)\n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "# The mean squared error\n",
    "print('Mean squared error: %.2f'\n",
    "      % mean_squared_error(y_test, y_pred))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.2f'\n",
    "      % r2_score(y_test, y_pred))\n",
    "# The intercept\n",
    "print(\"Intercept: \", regr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2:  0.8267442336725173 n_estimators =  100\n",
      "R^2:  0.8269001432672356 n_estimators =  200\n",
      "R^2:  0.8033690081685231 n_estimators =  300\n",
      "R^2:  0.8153509296583379 n_estimators =  400\n",
      "R^2:  0.8358221418687923 n_estimators =  500\n",
      "R^2:  0.8343661138684478 n_estimators =  600\n",
      "R^2:  0.8264447208865032 n_estimators =  700\n",
      "R^2:  0.8276493685349334 n_estimators =  800\n",
      "R^2:  0.8280775254753218 n_estimators =  900\n",
      "R^2:  0.8289334011134278 n_estimators =  1000\n"
     ]
    }
   ],
   "source": [
    "for i in range(100, 1100, 100):\n",
    "    text_classifier = RandomForestRegressor(n_estimators=i, random_state=0)\n",
    "    text_classifier.fit(X_train, y_train)\n",
    "    y_pred = text_classifier.predict(X_test)\n",
    "    print(\"R^2: \", metrics.r2_score(y_test, y_pred), \"n_estimators = \", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2:  0.8358221418687923\n",
      "Variance Score:  0.8638054554403909\n",
      "Max Error:  0.02051318894799975\n",
      "Mean Absolute Error: 0.006089691149809491\n",
      "Mean Squared Error: 5.9075178854207465e-05\n",
      "Root Mean Squared Error: 0.007686037916521585\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "\n",
    "text_classifier = RandomForestRegressor(n_estimators=500, random_state=0)\n",
    "text_classifier.fit(X_train, y_train)\n",
    "y_pred = text_classifier.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"R^2: \", metrics.r2_score(y_test, y_pred))\n",
    "print(\"Variance Score: \", metrics.explained_variance_score(y_test, y_pred))\n",
    "print(\"Max Error: \", metrics.max_error(y_test, y_pred))\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected n_neighbors <= n_samples,  but n_samples = 6, n_neighbors = 7",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-146-977174ac9dfc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#fit the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mpred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#make prediction on test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#calculate rmse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mrmse_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#store rmse values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\regression.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m         \u001b[0mneigh_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneigh_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    413\u001b[0m                 \u001b[1;34m\"Expected n_neighbors <= n_samples, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m                 \u001b[1;34m\" but n_samples = %d, n_neighbors = %d\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 415\u001b[1;33m                 \u001b[1;33m(\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    416\u001b[0m             )\n\u001b[0;32m    417\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected n_neighbors <= n_samples,  but n_samples = 6, n_neighbors = 7"
     ]
    }
   ],
   "source": [
    "from sklearn import neighbors\n",
    "from sklearn.metrics import mean_squared_error \n",
    "from math import sqrt\n",
    "\n",
    "rmse_val = [] #to store rmse values for different k\n",
    "for K in range(20):\n",
    "    K = K+1\n",
    "    model = neighbors.KNeighborsRegressor(n_neighbors = K)\n",
    "\n",
    "    model.fit(X_train, y_train)  #fit the model\n",
    "    pred=model.predict(X_test) #make prediction on test set\n",
    "    error = sqrt(mean_squared_error(y_test, pred)) #calculate rmse\n",
    "    rmse_val.append(error) #store rmse values\n",
    "    \n",
    "rmse_val = pd.DataFrame(rmse_val)\n",
    "print(rmse_val)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected n_neighbors <= n_samples,  but n_samples = 6, n_neighbors = 7",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-145-58593c66686e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#fit the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mpred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#make prediction on test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#calculate rmse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mrmse_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#store rmse values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\regression.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m         \u001b[0mneigh_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneigh_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    413\u001b[0m                 \u001b[1;34m\"Expected n_neighbors <= n_samples, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m                 \u001b[1;34m\" but n_samples = %d, n_neighbors = %d\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 415\u001b[1;33m                 \u001b[1;33m(\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    416\u001b[0m             )\n\u001b[0;32m    417\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected n_neighbors <= n_samples,  but n_samples = 6, n_neighbors = 7"
     ]
    }
   ],
   "source": [
    "from sklearn import neighbors\n",
    "from sklearn.metrics import mean_squared_error \n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "\n",
    "rmse_val = [] #to store rmse values for different k\n",
    "for K in range(7):\n",
    "    K = K+1\n",
    "    model = neighbors.KNeighborsRegressor(n_neighbors = K)\n",
    "\n",
    "    model.fit(X_train, y_train)  #fit the model\n",
    "    pred=model.predict(X_test) #make prediction on test set\n",
    "    error = sqrt(mean_squared_error(y_test, pred)) #calculate rmse\n",
    "    rmse_val.append(error) #store rmse values\n",
    "    \n",
    "rmse_val = pd.DataFrame(rmse_val)\n",
    "# print(rmse_val)\n",
    "print(\"K = \",rmse_val.idxmin()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2:  -0.6438447696059946\n",
      "Variance Score:  -0.6378339367143591\n",
      "Max Error:  0.06326711700000001\n",
      "Mean Absolute Error: 0.02113478747169811\n",
      "Mean Squared Error: 0.0006601342430712627\n",
      "Root Mean Squared Error: 0.025693077726719752\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn import neighbors\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "knn = neighbors.KNeighborsRegressor(n_neighbors = 2)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"R^2: \", metrics.r2_score(y_test, y_pred))\n",
    "print(\"Variance Score: \", metrics.explained_variance_score(y_test, y_pred))\n",
    "print(\"Max Error: \", metrics.max_error(y_test, y_pred))\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot outputs\n",
    "# plt.scatter(TwitterSenti, y_test,  color='yellow')\n",
    "# plt.scatter(TwitterSenti, y_pred, color='blue')\n",
    "# plt.scatter(NewsSenti, y_pred, color='purple')\n",
    "# plt.plot(X_test, y_test, color = \"blue\")\n",
    "# plt.scatter(X_test, y_pred, color = \"yellow\")\n",
    "plt.plot(y_test[0:100], color = \"red\")\n",
    "plt.plot(y_pred[0:100], color = \"blue\")\n",
    "\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# print(X_test.shape, y_test.shape)\n",
    "# print(X_test.shape, y_pred.shape)\n",
    "\n",
    "# print(y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-0.020866</td>\n",
       "      <td>-0.105209</td>\n",
       "      <td>-0.084343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.300488</td>\n",
       "      <td>-0.112562</td>\n",
       "      <td>0.187926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.246803</td>\n",
       "      <td>0.097136</td>\n",
       "      <td>-0.149668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.868060</td>\n",
       "      <td>-0.083677</td>\n",
       "      <td>-0.951738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.461470</td>\n",
       "      <td>0.097136</td>\n",
       "      <td>-0.364334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.121431</td>\n",
       "      <td>-0.112562</td>\n",
       "      <td>-0.233993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.522953</td>\n",
       "      <td>0.097136</td>\n",
       "      <td>-0.425817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.090117</td>\n",
       "      <td>-0.105209</td>\n",
       "      <td>-0.195327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>-0.065902</td>\n",
       "      <td>-0.067132</td>\n",
       "      <td>-0.001230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>-0.353484</td>\n",
       "      <td>-0.112562</td>\n",
       "      <td>0.240923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Actual  Predicted  Difference\n",
       "0  -0.020866  -0.105209   -0.084343\n",
       "1  -0.300488  -0.112562    0.187926\n",
       "2   0.246803   0.097136   -0.149668\n",
       "3   0.868060  -0.083677   -0.951738\n",
       "4   0.461470   0.097136   -0.364334\n",
       "..       ...        ...         ...\n",
       "56  0.121431  -0.112562   -0.233993\n",
       "57  0.522953   0.097136   -0.425817\n",
       "58  0.090117  -0.105209   -0.195327\n",
       "59 -0.065902  -0.067132   -0.001230\n",
       "60 -0.353484  -0.112562    0.240923\n",
       "\n",
       "[61 rows x 3 columns]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred, 'Difference': (y_pred - y_test)})\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifier\n",
    "\n",
    "# df1 = pd.read_csv(r\"C:\\Users\\Jurgen\\Desktop\\FYP\\News\\SK-Learn\\News_Stock_Data_30.csv\")\n",
    "# X = df1[\"News_Senti\"].to_numpy().reshape(-1, 1)\n",
    "# y = df1[\"Stock_Changes\"].to_numpy()\n",
    "# df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i] == y_pred[i]:\n",
    "        count += 1\n",
    "print(count, '/', len(y_test))\n",
    "print(y_test)\n",
    "print(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
