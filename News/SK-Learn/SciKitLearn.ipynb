{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News_Senti</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Open_USD</th>\n",
       "      <th>High_USD</th>\n",
       "      <th>Low_USD</th>\n",
       "      <th>Adj_Close_USD</th>\n",
       "      <th>Twitter_Senti</th>\n",
       "      <th>Reddit_Comment</th>\n",
       "      <th>Reddit_SelfText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-0.013258</td>\n",
       "      <td>13954800.0</td>\n",
       "      <td>0.168756</td>\n",
       "      <td>0.168900</td>\n",
       "      <td>0.164937</td>\n",
       "      <td>0.161698</td>\n",
       "      <td>0.354322</td>\n",
       "      <td>0.195632</td>\n",
       "      <td>0.065820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.070484</td>\n",
       "      <td>12513900.0</td>\n",
       "      <td>0.168580</td>\n",
       "      <td>0.169250</td>\n",
       "      <td>0.168421</td>\n",
       "      <td>0.165144</td>\n",
       "      <td>0.354322</td>\n",
       "      <td>0.263295</td>\n",
       "      <td>0.223880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.033616</td>\n",
       "      <td>20286800.0</td>\n",
       "      <td>0.166439</td>\n",
       "      <td>0.168949</td>\n",
       "      <td>0.166439</td>\n",
       "      <td>0.164714</td>\n",
       "      <td>0.354322</td>\n",
       "      <td>0.309500</td>\n",
       "      <td>-0.189340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.061636</td>\n",
       "      <td>67935700.0</td>\n",
       "      <td>0.178142</td>\n",
       "      <td>0.180494</td>\n",
       "      <td>0.176514</td>\n",
       "      <td>0.176358</td>\n",
       "      <td>0.354322</td>\n",
       "      <td>0.316293</td>\n",
       "      <td>-0.402100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.120139</td>\n",
       "      <td>62404000.0</td>\n",
       "      <td>0.177505</td>\n",
       "      <td>0.184407</td>\n",
       "      <td>0.177301</td>\n",
       "      <td>0.179556</td>\n",
       "      <td>0.354322</td>\n",
       "      <td>0.034783</td>\n",
       "      <td>0.420870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>-0.036621</td>\n",
       "      <td>25425400.0</td>\n",
       "      <td>0.184792</td>\n",
       "      <td>0.185902</td>\n",
       "      <td>0.183966</td>\n",
       "      <td>0.181719</td>\n",
       "      <td>0.354322</td>\n",
       "      <td>0.073471</td>\n",
       "      <td>-0.102990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.092705</td>\n",
       "      <td>25587400.0</td>\n",
       "      <td>0.186901</td>\n",
       "      <td>0.187061</td>\n",
       "      <td>0.184615</td>\n",
       "      <td>0.180922</td>\n",
       "      <td>0.354322</td>\n",
       "      <td>0.234032</td>\n",
       "      <td>0.261930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.101774</td>\n",
       "      <td>23492600.0</td>\n",
       "      <td>0.187136</td>\n",
       "      <td>0.187359</td>\n",
       "      <td>0.185055</td>\n",
       "      <td>0.182515</td>\n",
       "      <td>-0.102700</td>\n",
       "      <td>0.199300</td>\n",
       "      <td>0.102714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.052211</td>\n",
       "      <td>24611200.0</td>\n",
       "      <td>0.183645</td>\n",
       "      <td>0.185186</td>\n",
       "      <td>0.183033</td>\n",
       "      <td>0.180445</td>\n",
       "      <td>0.354322</td>\n",
       "      <td>0.228048</td>\n",
       "      <td>0.251273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.222372</td>\n",
       "      <td>27390100.0</td>\n",
       "      <td>0.204382</td>\n",
       "      <td>0.205071</td>\n",
       "      <td>0.202789</td>\n",
       "      <td>0.200612</td>\n",
       "      <td>0.354322</td>\n",
       "      <td>0.138369</td>\n",
       "      <td>0.299167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.179139</td>\n",
       "      <td>33333000.0</td>\n",
       "      <td>0.204052</td>\n",
       "      <td>0.204658</td>\n",
       "      <td>0.200585</td>\n",
       "      <td>0.198478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.123221</td>\n",
       "      <td>-0.052550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.310380</td>\n",
       "      <td>28654800.0</td>\n",
       "      <td>0.205414</td>\n",
       "      <td>0.208468</td>\n",
       "      <td>0.205173</td>\n",
       "      <td>0.203440</td>\n",
       "      <td>0.354322</td>\n",
       "      <td>0.210979</td>\n",
       "      <td>0.533933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.199303</td>\n",
       "      <td>32042000.0</td>\n",
       "      <td>0.204459</td>\n",
       "      <td>0.205850</td>\n",
       "      <td>0.200871</td>\n",
       "      <td>0.198305</td>\n",
       "      <td>0.354322</td>\n",
       "      <td>0.054587</td>\n",
       "      <td>0.273650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>-0.057383</td>\n",
       "      <td>33580500.0</td>\n",
       "      <td>0.202036</td>\n",
       "      <td>0.202434</td>\n",
       "      <td>0.195495</td>\n",
       "      <td>0.195160</td>\n",
       "      <td>0.354322</td>\n",
       "      <td>0.179833</td>\n",
       "      <td>0.165186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.153396</td>\n",
       "      <td>29663900.0</td>\n",
       "      <td>0.195477</td>\n",
       "      <td>0.197756</td>\n",
       "      <td>0.193709</td>\n",
       "      <td>0.193261</td>\n",
       "      <td>0.354322</td>\n",
       "      <td>0.383969</td>\n",
       "      <td>0.199500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.185523</td>\n",
       "      <td>26891000.0</td>\n",
       "      <td>0.196563</td>\n",
       "      <td>0.199753</td>\n",
       "      <td>0.195341</td>\n",
       "      <td>0.195767</td>\n",
       "      <td>0.354322</td>\n",
       "      <td>0.062768</td>\n",
       "      <td>-0.250740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.181762</td>\n",
       "      <td>38358900.0</td>\n",
       "      <td>0.190121</td>\n",
       "      <td>0.193250</td>\n",
       "      <td>0.189893</td>\n",
       "      <td>0.188359</td>\n",
       "      <td>0.354322</td>\n",
       "      <td>0.207924</td>\n",
       "      <td>-0.229400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.156127</td>\n",
       "      <td>91328700.0</td>\n",
       "      <td>0.187458</td>\n",
       "      <td>0.191126</td>\n",
       "      <td>0.183772</td>\n",
       "      <td>0.182223</td>\n",
       "      <td>0.354322</td>\n",
       "      <td>0.334719</td>\n",
       "      <td>0.079950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.047075</td>\n",
       "      <td>25362600.0</td>\n",
       "      <td>0.188012</td>\n",
       "      <td>0.188138</td>\n",
       "      <td>0.185120</td>\n",
       "      <td>0.183915</td>\n",
       "      <td>1.631300</td>\n",
       "      <td>0.130219</td>\n",
       "      <td>0.040545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>-0.023346</td>\n",
       "      <td>43098400.0</td>\n",
       "      <td>0.153150</td>\n",
       "      <td>0.155843</td>\n",
       "      <td>0.151955</td>\n",
       "      <td>0.153483</td>\n",
       "      <td>0.354322</td>\n",
       "      <td>0.202148</td>\n",
       "      <td>0.054500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.270494</td>\n",
       "      <td>62026000.0</td>\n",
       "      <td>0.146427</td>\n",
       "      <td>0.150944</td>\n",
       "      <td>0.144945</td>\n",
       "      <td>0.148281</td>\n",
       "      <td>0.197900</td>\n",
       "      <td>0.338811</td>\n",
       "      <td>0.625450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.210893</td>\n",
       "      <td>91312200.0</td>\n",
       "      <td>0.127730</td>\n",
       "      <td>0.129274</td>\n",
       "      <td>0.125974</td>\n",
       "      <td>0.124275</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.252168</td>\n",
       "      <td>0.358890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.098321</td>\n",
       "      <td>41025300.0</td>\n",
       "      <td>0.132775</td>\n",
       "      <td>0.134781</td>\n",
       "      <td>0.131852</td>\n",
       "      <td>0.131850</td>\n",
       "      <td>0.401900</td>\n",
       "      <td>0.019361</td>\n",
       "      <td>0.217208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.079191</td>\n",
       "      <td>27023200.0</td>\n",
       "      <td>0.136769</td>\n",
       "      <td>0.137502</td>\n",
       "      <td>0.135543</td>\n",
       "      <td>0.134224</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.274506</td>\n",
       "      <td>-0.566967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.232194</td>\n",
       "      <td>32668100.0</td>\n",
       "      <td>0.149470</td>\n",
       "      <td>0.151278</td>\n",
       "      <td>0.148547</td>\n",
       "      <td>0.146869</td>\n",
       "      <td>0.608100</td>\n",
       "      <td>0.221375</td>\n",
       "      <td>0.119625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.093342</td>\n",
       "      <td>25886200.0</td>\n",
       "      <td>0.154730</td>\n",
       "      <td>0.155503</td>\n",
       "      <td>0.153496</td>\n",
       "      <td>0.153700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.147463</td>\n",
       "      <td>0.240143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.128877</td>\n",
       "      <td>19114300.0</td>\n",
       "      <td>0.171256</td>\n",
       "      <td>0.172645</td>\n",
       "      <td>0.169806</td>\n",
       "      <td>0.170228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.272980</td>\n",
       "      <td>0.251909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    News_Senti      Volume  Open_USD  High_USD   Low_USD  Adj_Close_USD  \\\n",
       "0    -0.013258  13954800.0  0.168756  0.168900  0.164937       0.161698   \n",
       "1     0.070484  12513900.0  0.168580  0.169250  0.168421       0.165144   \n",
       "2     0.033616  20286800.0  0.166439  0.168949  0.166439       0.164714   \n",
       "3     0.061636  67935700.0  0.178142  0.180494  0.176514       0.176358   \n",
       "4     0.120139  62404000.0  0.177505  0.184407  0.177301       0.179556   \n",
       "5    -0.036621  25425400.0  0.184792  0.185902  0.183966       0.181719   \n",
       "6     0.092705  25587400.0  0.186901  0.187061  0.184615       0.180922   \n",
       "7     0.101774  23492600.0  0.187136  0.187359  0.185055       0.182515   \n",
       "8     0.052211  24611200.0  0.183645  0.185186  0.183033       0.180445   \n",
       "9     0.222372  27390100.0  0.204382  0.205071  0.202789       0.200612   \n",
       "10    0.179139  33333000.0  0.204052  0.204658  0.200585       0.198478   \n",
       "11    0.310380  28654800.0  0.205414  0.208468  0.205173       0.203440   \n",
       "12    0.199303  32042000.0  0.204459  0.205850  0.200871       0.198305   \n",
       "13   -0.057383  33580500.0  0.202036  0.202434  0.195495       0.195160   \n",
       "14    0.153396  29663900.0  0.195477  0.197756  0.193709       0.193261   \n",
       "15    0.185523  26891000.0  0.196563  0.199753  0.195341       0.195767   \n",
       "16    0.181762  38358900.0  0.190121  0.193250  0.189893       0.188359   \n",
       "17    0.156127  91328700.0  0.187458  0.191126  0.183772       0.182223   \n",
       "18    0.047075  25362600.0  0.188012  0.188138  0.185120       0.183915   \n",
       "19   -0.023346  43098400.0  0.153150  0.155843  0.151955       0.153483   \n",
       "20    0.270494  62026000.0  0.146427  0.150944  0.144945       0.148281   \n",
       "21    0.210893  91312200.0  0.127730  0.129274  0.125974       0.124275   \n",
       "22    0.098321  41025300.0  0.132775  0.134781  0.131852       0.131850   \n",
       "23    0.079191  27023200.0  0.136769  0.137502  0.135543       0.134224   \n",
       "24    0.232194  32668100.0  0.149470  0.151278  0.148547       0.146869   \n",
       "25    0.093342  25886200.0  0.154730  0.155503  0.153496       0.153700   \n",
       "26    0.128877  19114300.0  0.171256  0.172645  0.169806       0.170228   \n",
       "\n",
       "    Twitter_Senti  Reddit_Comment  Reddit_SelfText  \n",
       "0        0.354322        0.195632         0.065820  \n",
       "1        0.354322        0.263295         0.223880  \n",
       "2        0.354322        0.309500        -0.189340  \n",
       "3        0.354322        0.316293        -0.402100  \n",
       "4        0.354322        0.034783         0.420870  \n",
       "5        0.354322        0.073471        -0.102990  \n",
       "6        0.354322        0.234032         0.261930  \n",
       "7       -0.102700        0.199300         0.102714  \n",
       "8        0.354322        0.228048         0.251273  \n",
       "9        0.354322        0.138369         0.299167  \n",
       "10       0.000000        0.123221        -0.052550  \n",
       "11       0.354322        0.210979         0.533933  \n",
       "12       0.354322        0.054587         0.273650  \n",
       "13       0.354322        0.179833         0.165186  \n",
       "14       0.354322        0.383969         0.199500  \n",
       "15       0.354322        0.062768        -0.250740  \n",
       "16       0.354322        0.207924        -0.229400  \n",
       "17       0.354322        0.334719         0.079950  \n",
       "18       1.631300        0.130219         0.040545  \n",
       "19       0.354322        0.202148         0.054500  \n",
       "20       0.197900        0.338811         0.625450  \n",
       "21       0.000000        0.252168         0.358890  \n",
       "22       0.401900        0.019361         0.217208  \n",
       "23       0.340000        0.274506        -0.566967  \n",
       "24       0.608100        0.221375         0.119625  \n",
       "25       0.000000        0.147463         0.240143  \n",
       "26       0.000000        0.272980         0.251909  "
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\Jurgen\\Desktop\\FYP\\Shared\\News_Twitter_Stock_Reddit_NaN.csv\")\n",
    "# df = df.drop(columns = [\"Reddit_SelfText\", \"Twitter_Senti\", \"News_Senti\"])\n",
    "# df = df.fillna(0)\n",
    "df = df.drop(['Date', 'Stockname', \"Stock_Difference\", \"Close_USD\",], axis=1)\n",
    "# 'News_Senti', \"Twitter_Senti\", \"Reddit_Comment\", \"Reddit_SelfText\",\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "df.shape\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "X = df.loc[:, df.columns != 'Adj_Close_USD'].to_numpy()\n",
    "y = df[\"Adj_Close_USD\"].to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "\n",
    "print(X_train.shape[0])\n",
    "print(X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.96699045 n_estimators = 100\n",
      "R^2: 0.97210357 n_estimators = 200\n",
      "R^2: 0.97268673 n_estimators = 300\n",
      "R^2: 0.97224172 n_estimators = 400\n",
      "R^2: 0.97276894 n_estimators = 500\n",
      "R^2: 0.97458742 n_estimators = 600\n",
      "R^2: 0.97351347 n_estimators = 700\n",
      "R^2: 0.97183798 n_estimators = 800\n",
      "R^2: 0.97141377 n_estimators = 900\n",
      "R^2: 0.97090352 n_estimators = 1000\n",
      "\n",
      "N_Estimators:  600\n",
      "R^2:  0.974587420792614\n",
      "Variance Score:  0.9752332618073147\n",
      "Max Error:  0.007351628143444561\n",
      "Mean Absolute Error: 0.00270896149542891\n",
      "Mean Squared Error: 1.123556719209576e-05\n",
      "Root Mean Squared Error: 0.0033519497597809785\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "\n",
    "r2_value = []\n",
    "for i in range(100, 1100, 100):\n",
    "    text_classifier = RandomForestRegressor(n_estimators=i, random_state=0)\n",
    "    text_classifier.fit(X_train, y_train)\n",
    "    y_pred = text_classifier.predict(X_test)\n",
    "    r2_value.append((metrics.r2_score(y_test, y_pred),i))\n",
    "    print(\"R^2: {:.8f} n_estimators = {}\".format(metrics.r2_score(y_test, y_pred), i))\n",
    "\n",
    "r2 = pd.DataFrame(r2_value, columns = [\"R2\", \"n_estimators\"]).set_index(\"n_estimators\")\n",
    "n_est = r2.R2[r2.R2 == r2.R2.max()].index[0]\n",
    "\n",
    "\n",
    "text_classifier = RandomForestRegressor(n_estimators = n_est, random_state=0)\n",
    "text_classifier.fit(X_train, y_train)\n",
    "y_pred = text_classifier.predict(X_test)\n",
    "\n",
    "print()\n",
    "print(\"N_Estimators: \", n_est)\n",
    "print(\"R^2: \", metrics.r2_score(y_test, y_pred))\n",
    "print(\"Variance Score: \", metrics.explained_variance_score(y_test, y_pred))\n",
    "print(\"Max Error: \", metrics.max_error(y_test, y_pred))\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot outputs\n",
    "\n",
    "plt.plot(y_test, color = \"red\")\n",
    "plt.plot(y_pred, color = \"blue\")\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.164714</td>\n",
       "      <td>0.167083</td>\n",
       "      <td>0.236855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.146869</td>\n",
       "      <td>0.150311</td>\n",
       "      <td>0.344248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.193261</td>\n",
       "      <td>0.193731</td>\n",
       "      <td>0.046957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.182223</td>\n",
       "      <td>0.181695</td>\n",
       "      <td>-0.052769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.181719</td>\n",
       "      <td>0.180733</td>\n",
       "      <td>-0.098622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.203440</td>\n",
       "      <td>0.198143</td>\n",
       "      <td>-0.529735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.134224</td>\n",
       "      <td>0.141576</td>\n",
       "      <td>0.735163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.195160</td>\n",
       "      <td>0.192375</td>\n",
       "      <td>-0.278499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.153483</td>\n",
       "      <td>0.154282</td>\n",
       "      <td>0.079840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.148281</td>\n",
       "      <td>0.143759</td>\n",
       "      <td>-0.452206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.188359</td>\n",
       "      <td>0.186024</td>\n",
       "      <td>-0.233521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.165144</td>\n",
       "      <td>0.165936</td>\n",
       "      <td>0.079157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.198478</td>\n",
       "      <td>0.196303</td>\n",
       "      <td>-0.217481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.170228</td>\n",
       "      <td>0.166153</td>\n",
       "      <td>-0.407493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Actual  Predicted  Difference\n",
       "0   0.164714   0.167083    0.236855\n",
       "1   0.146869   0.150311    0.344248\n",
       "2   0.193261   0.193731    0.046957\n",
       "3   0.182223   0.181695   -0.052769\n",
       "4   0.181719   0.180733   -0.098622\n",
       "5   0.203440   0.198143   -0.529735\n",
       "6   0.134224   0.141576    0.735163\n",
       "7   0.195160   0.192375   -0.278499\n",
       "8   0.153483   0.154282    0.079840\n",
       "9   0.148281   0.143759   -0.452206\n",
       "10  0.188359   0.186024   -0.233521\n",
       "11  0.165144   0.165936    0.079157\n",
       "12  0.198478   0.196303   -0.217481\n",
       "13  0.170228   0.166153   -0.407493"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred, 'Difference': (y_pred - y_test) * 100})\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "regr = LinearRegression()\n",
    "regr.fit(X_train, y_train)\n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "# The mean squared error\n",
    "print('Mean squared error: %.2f'\n",
    "      % mean_squared_error(y_test, y_pred))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.2f'\n",
    "      % r2_score(y_test, y_pred))\n",
    "# The intercept\n",
    "print(\"Intercept: \", regr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "from sklearn.metrics import mean_squared_error \n",
    "from math import sqrt\n",
    "\n",
    "rmse_val = [] #to store rmse values for different k\n",
    "for K in range(20):\n",
    "    K = K+1\n",
    "    model = neighbors.KNeighborsRegressor(n_neighbors = K)\n",
    "\n",
    "    model.fit(X_train, y_train)  #fit the model\n",
    "    pred=model.predict(X_test) #make prediction on test set\n",
    "    error = sqrt(mean_squared_error(y_test, pred)) #calculate rmse\n",
    "    rmse_val.append(error) #store rmse values\n",
    "    \n",
    "rmse_val = pd.DataFrame(rmse_val)\n",
    "print(rmse_val)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "from sklearn.metrics import mean_squared_error \n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "\n",
    "rmse_val = [] #to store rmse values for different k\n",
    "for K in range(7):\n",
    "    K = K+1\n",
    "    model = neighbors.KNeighborsRegressor(n_neighbors = K)\n",
    "\n",
    "    model.fit(X_train, y_train)  #fit the model\n",
    "    pred=model.predict(X_test) #make prediction on test set\n",
    "    error = sqrt(mean_squared_error(y_test, pred)) #calculate rmse\n",
    "    rmse_val.append(error) #store rmse values\n",
    "    \n",
    "rmse_val = pd.DataFrame(rmse_val)\n",
    "# print(rmse_val)\n",
    "print(\"K = \",rmse_val.idxmin()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn import neighbors\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "knn = neighbors.KNeighborsRegressor(n_neighbors = 2)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"R^2: \", metrics.r2_score(y_test, y_pred))\n",
    "print(\"Variance Score: \", metrics.explained_variance_score(y_test, y_pred))\n",
    "print(\"Max Error: \", metrics.max_error(y_test, y_pred))\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifier\n",
    "\n",
    "# df1 = pd.read_csv(r\"C:\\Users\\Jurgen\\Desktop\\FYP\\News\\SK-Learn\\News_Stock_Data_30.csv\")\n",
    "# X = df1[\"News_Senti\"].to_numpy().reshape(-1, 1)\n",
    "# y = df1[\"Stock_Changes\"].to_numpy()\n",
    "# df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i] == y_pred[i]:\n",
    "        count += 1\n",
    "print(count, '/', len(y_test))\n",
    "print(y_test)\n",
    "print(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
