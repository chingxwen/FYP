{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from pprint import pprint\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import FreqDist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading of .csv file\n",
    "df = pd.read_csv(\"TheGuardian.csv\")\n",
    "content = df['Text']\n",
    "assert content.isnull().count() == len(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before stop word removal count\n",
    "count = [len(content[i]) for i in range(len(content))]\n",
    "# data['Count Before'] = count\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "data = []\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "content = content.apply(lambda x: tokenizer.tokenize(x))\n",
    "content.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removal of Stop Words\n",
    "from nltk.corpus import stopwords\n",
    "set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = [w for w in text if w not in stopwords.words('english')]\n",
    "    return words\n",
    "\n",
    "content = content.apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def word_lemmatizer(text):\n",
    "    lem_text = [lemmatizer.lemmatize(i, pos=\"v\") for i in text]\n",
    "    return lem_text\n",
    "\n",
    "content = content.apply(lambda x: word_lemmatizer(x))\n",
    "\n",
    "content_freq = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count after removal of stop words\n",
    "countaft = [len(content[i]) for i in range(len(content))]\n",
    "print(countaft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tagging\n",
    "content_tag = [nltk.pos_tag(i) for i in content]\n",
    "print(content_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Difference for the count before and after stop word removal\n",
    "count_diff = []\n",
    "for i in range(len(count)):\n",
    "    count_diff.append(round(((count[i] - countaft[i])/count[i])*100, 1))\n",
    "print(count_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(content_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Freq = [FreqDist(content_freq[i]).most_common(5) for i in range(len(content_freq))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Content with Tag'] = content_tag\n",
    "df['Amount Cleansed (%)'] = count_diff\n",
    "df['Frequency'] = Freq\n",
    "\n",
    "# df.to_csv('Output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feeling_words = []\n",
    "word_total_count = 0\n",
    "for articles in content_tag:\n",
    "    word_total_count += len(articles)\n",
    "    for words in articles:\n",
    "        if words[1].startswith('JJ'):\n",
    "            feeling_words.append((words[0].upper()))\n",
    "\n",
    "Feeling_Freq = FreqDist(feeling_words).most_common(500)\n",
    "pprint(Feeling_Freq)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature\n",
    "\n",
    "word_features = feeling_words\n",
    "\n",
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(find_features(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#useless\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "matrix = CountVectorizer(max_features=1000)\n",
    "X = matrix.fit_transform(content).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a frequency distribution of each adjectives.\n",
    "Freq = [FreqDist(content_freq[i]).most_common(5) for i in range(len(content_freq))]\n",
    "\n",
    "# listing the 5000 most frequent words\n",
    "word_features = list(Freq.keys())[:5000]\n",
    "\n",
    "# function to create a dictionary of features for each review in the list document.\n",
    "# The keys are the words in word_features \n",
    "# The values of each key are either true or false for wether that feature appears in the review or not\n",
    "\n",
    "def find_features(document):\n",
    "    words = word_tokenize(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "    return features\n",
    "\n",
    "# Creating features for each review\n",
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]\n",
    "\n",
    "# Shuffling the documents \n",
    "random.shuffle(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
