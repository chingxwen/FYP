{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from pprint import pprint\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import FreqDist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading of .csv file\n",
    "df = pd.read_csv(\"TheGuardian.csv\")\n",
    "content = df['Text']\n",
    "assert content.isnull().count() == len(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4161, 8558, 9216, 2630, 13088, 1884, 7798, 11877, 8701, 5545, 9089, 3172, 2396, 8622, 9104, 12339, 11246, 25259, 7311, 12420, 12800, 3761, 22898, 7330, 7668, 4314, 10137, 11805, 10119, 10701, 5143, 13380, 992, 6623, 3843, 4426, 9994, 7465, 2940, 3389, 6841, 7265, 5986, 2897, 1600, 996, 3660, 8850, 6196, 7948, 16452, 7606, 7387, 8633, 8508, 7323, 4053, 8262, 2294, 1864, 7465, 5714, 6585, 634, 7773, 7809, 620, 19996, 10152, 5565, 4053, 3195, 7264, 6493, 3323, 7238, 4246, 9153, 9569, 2993, 1574, 1109, 4083, 6458, 4632, 3566, 10106, 3529, 5727, 9729, 6723, 3323, 814, 5145, 4136, 1189, 1456, 3394, 2701, 7731, 9246, 12075, 5319, 5235, 7716, 3703, 3352, 3319, 4386, 7743, 1790, 6435, 2288, 3964, 9600, 7561, 11738, 716, 5865, 3308, 7645, 5231, 9151, 13346, 5858, 5543, 8829, 2196, 1932, 26457, 3181, 21210, 5148, 4563, 8181, 1308, 4879, 4669, 3967, 7892, 6855, 5642, 4210, 6552, 13383, 5731, 6321, 5959, 7393, 9034, 16574, 8440, 7450, 2785, 11192, 4626, 3390, 9233, 6371, 9079, 7287, 10079, 3723, 6912, 5944, 2533, 6999, 3163, 7094, 9438, 3627, 8302, 9808, 12005, 7609, 2314, 5122, 6949, 8686, 6842, 7764, 31560, 8570, 8006, 1849, 3897, 5877, 3280, 4450, 6573, 4550, 10617, 1329, 2030, 3932, 5076, 268, 4166, 14084, 1937]\n"
     ]
    }
   ],
   "source": [
    "#Before stop word removal count\n",
    "count = [len(content[i]) for i in range(len(content))]\n",
    "# data['Count Before'] = count\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [Google, has, snapped, up, the, Fitbit, activi...\n",
       "1     [Brilliant, camera, slick, features, and, smal...\n",
       "2     [On, a, dark, dark, night, in, a, dark, dark, ...\n",
       "3     [Labour, has, written, to, the, competition, r...\n",
       "4     [Google, s, latest, Pixel, 4, XL, smartphone, ...\n",
       "5     [Apple, has, launched, a, new, set, of, its, p...\n",
       "6     [I, use, a, 9, 7in, Samsung, Galaxy, Tab, S2, ...\n",
       "7     [Always, on, screen, completes, the, package, ...\n",
       "8     [Still, an, absolute, beast, in, every, way, e...\n",
       "9     [First, it, was, books, then, it, was, the, Ki...\n",
       "10    [The, OnePlus, 7T, takes, the, best, bits, of,...\n",
       "11    [In, a, country, where, the, biggest, companie...\n",
       "12    [I, think, you, should, at, least, make, an, e...\n",
       "13    [Plastic, packaging, has, many, amazing, quali...\n",
       "14    [A, great, camera, screen, and, performance, c...\n",
       "15    [After, a, period, of, stagnation, the, A, Lea...\n",
       "16    [Apple, s, iPhone, has, gone, pro, for, its, 1...\n",
       "17    [Gordon, s, wine, bar, is, reached, through, a...\n",
       "18    [Microsoft, has, launched, the, Surface, Pro, ...\n",
       "19    [Apple, s, lower, cost, model, has, latest, ch...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenization\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "data = []\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "content = content.apply(lambda x: tokenizer.tokenize(x))\n",
    "content.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removal of Stop Words\n",
    "from nltk.corpus import stopwords\n",
    "set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = [w for w in text if w not in stopwords.words('english')]\n",
    "    return words\n",
    "\n",
    "content = content.apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def word_lemmatizer(text):\n",
    "    lem_text = [lemmatizer.lemmatize(i, pos=\"v\") for i in text]\n",
    "    return lem_text\n",
    "\n",
    "content = content.apply(lambda x: word_lemmatizer(x))\n",
    "\n",
    "content_freq = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[435, 944, 951, 241, 1399, 202, 834, 1237, 973, 558, 1007, 331, 255, 853, 1047, 1320, 1240, 2484, 802, 1348, 1366, 406, 2427, 710, 820, 468, 1057, 1288, 1063, 1156, 616, 1389, 112, 664, 383, 476, 1096, 754, 301, 317, 793, 766, 563, 320, 169, 96, 368, 861, 569, 876, 1827, 846, 809, 970, 879, 811, 393, 870, 253, 206, 801, 548, 709, 70, 835, 851, 67, 1956, 1132, 596, 424, 378, 824, 732, 367, 796, 455, 935, 995, 328, 171, 107, 432, 604, 441, 349, 1140, 345, 581, 1004, 723, 347, 78, 545, 449, 131, 148, 365, 298, 885, 919, 1288, 548, 556, 853, 407, 343, 345, 479, 792, 173, 686, 224, 394, 1034, 789, 1170, 82, 637, 341, 797, 579, 913, 1437, 580, 558, 892, 240, 205, 2776, 307, 1985, 545, 483, 807, 137, 494, 480, 426, 818, 731, 616, 428, 705, 1570, 591, 649, 628, 724, 904, 1585, 888, 806, 309, 1235, 550, 329, 1030, 705, 921, 799, 1094, 429, 762, 559, 260, 765, 329, 779, 1047, 343, 862, 960, 1240, 815, 230, 531, 779, 827, 664, 876, 3000, 937, 836, 199, 428, 641, 329, 486, 671, 458, 1089, 136, 220, 416, 502, 29, 443, 1447, 204]\n"
     ]
    }
   ],
   "source": [
    "#Count after removal of stop words\n",
    "countaft = [len(content[i]) for i in range(len(content))]\n",
    "print(countaft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'isdigit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-a69735ab7918>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Tagging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-a69735ab7918>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Tagging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    160\u001b[0m     \"\"\"\n\u001b[0;32m    161\u001b[0m     \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py\u001b[0m in \u001b[0;36m_pos_tag\u001b[1;34m(tokens, tagset, tagger, lang)\u001b[0m\n\u001b[0;32m    117\u001b[0m         )\n\u001b[0;32m    118\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0mtagged_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Maps to the specified tagset.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlang\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'eng'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tag\\perceptron.py\u001b[0m in \u001b[0;36mtag\u001b[1;34m(self, tokens, return_conf, use_tagdict)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSTART\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEND\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m             \u001b[0mtag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtagdict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0muse_tagdict\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tag\\perceptron.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSTART\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEND\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m             \u001b[0mtag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtagdict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0muse_tagdict\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tag\\perceptron.py\u001b[0m in \u001b[0;36mnormalize\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'-'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'-'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;34m'!HYPHEN'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m         \u001b[1;32melif\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    260\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;34m'!YEAR'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'isdigit'"
     ]
    }
   ],
   "source": [
    "#Tagging\n",
    "content = [nltk.pos_tag(i) for i in content]\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[89.5, 89.0, 89.7, 90.8, 89.3, 89.3, 89.3, 89.6, 88.8, 89.9, 88.9, 89.6, 89.4, 90.1, 88.5, 89.3, 89.0, 90.2, 89.0, 89.1, 89.3, 89.2, 89.4, 90.3, 89.3, 89.2, 89.6, 89.1, 89.5, 89.2, 88.0, 89.6, 88.7, 90.0, 90.0, 89.2, 89.0, 89.9, 89.8, 90.6, 88.4, 89.5, 90.6, 89.0, 89.4, 90.4, 89.9, 90.3, 90.8, 89.0, 88.9, 88.9, 89.0, 88.8, 89.7, 88.9, 90.3, 89.5, 89.0, 88.9, 89.3, 90.4, 89.2, 89.0, 89.3, 89.1, 89.2, 90.2, 88.8, 89.3, 89.5, 88.2, 88.7, 88.7, 89.0, 89.0, 89.3, 89.8, 89.6, 89.0, 89.1, 90.4, 89.4, 90.6, 90.5, 90.2, 88.7, 90.2, 89.9, 89.7, 89.2, 89.6, 90.4, 89.4, 89.1, 89.0, 89.8, 89.2, 89.0, 88.6, 90.1, 89.3, 89.7, 89.4, 88.9, 89.0, 89.8, 89.6, 89.1, 89.8, 90.3, 89.3, 90.2, 90.1, 89.2, 89.6, 90.0, 88.5, 89.1, 89.7, 89.6, 88.9, 90.0, 89.2, 90.1, 89.9, 89.9, 89.1, 89.4, 89.5, 90.3, 90.6, 89.4, 89.4, 90.1, 89.5, 89.9, 89.7, 89.3, 89.6, 89.3, 89.1, 89.8, 89.2, 88.3, 89.7, 89.7, 89.5, 90.2, 90.0, 90.4, 89.5, 89.2, 88.9, 89.0, 88.1, 90.3, 88.8, 88.9, 89.9, 89.0, 89.1, 88.5, 89.0, 90.6, 89.7, 89.1, 89.6, 89.0, 88.9, 90.5, 89.6, 90.2, 89.7, 89.3, 90.1, 89.6, 88.8, 90.5, 90.3, 88.7, 90.5, 89.1, 89.6, 89.2, 89.0, 89.1, 90.0, 89.1, 89.8, 89.9, 89.7, 89.8, 89.2, 89.4, 90.1, 89.2, 89.4, 89.7, 89.5]\n"
     ]
    }
   ],
   "source": [
    "#Difference for the count before and after stop word removal\n",
    "count_diff = []\n",
    "for i in range(len(count)):\n",
    "    count_diff.append(round(((count[i] - countaft[i])/count[i])*100, 1))\n",
    "print(count_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      [Google, snap, Fitbit, activity, tracker, busi...\n",
      "1      [Brilliant, camera, slick, feature, small, siz...\n",
      "2      [On, dark, dark, night, dark, dark, town, dark...\n",
      "3      [Labour, write, competition, regulator, call, ...\n",
      "4      [Google, latest, Pixel, 4, XL, smartphone, bra...\n",
      "                             ...                        \n",
      "195    [In, 2007, coincide, moment, David, Beckham, m...\n",
      "196    [Weirdly, indelible, memory, lacy, trousers, R...\n",
      "197    [How, find, best, value, smartphone, deal, If,...\n",
      "198    [Thursday, 12, July, red, letter, day, thousan...\n",
      "199    [Glass, maker, Corning, unveil, new, version, ...\n",
      "Name: Text, Length: 200, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(content_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Freq = [FreqDist(content_freq[i]).most_common(5) for i in range(len(content_freq))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Amount Cleansed (%)'] = count_diff\n",
    "df['Frequency'] = Freq\n",
    "\n",
    "df.to_csv('Output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pprint.pprint(Freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.585455286209005\n",
      "[('SAMSUNG', 523),\n",
      " ('PRO', 508),\n",
      " ('APPLE', 507),\n",
      " ('GOOGLE', 480),\n",
      " ('HUAWEI', 476),\n",
      " ('ANDROID', 312),\n",
      " ('PIXEL', 276),\n",
      " ('GALAXY', 273),\n",
      " ('US', 265),\n",
      " ('A', 193),\n",
      " ('ONEPLUS', 179),\n",
      " ('XS', 169),\n",
      " ('S10', 168),\n",
      " ('HONOR', 149),\n",
      " ('CHINA', 147),\n",
      " ('UK', 134),\n",
      " ('MAX', 134),\n",
      " ('USB', 119),\n",
      " ('AMAZON', 117),\n",
      " ('NONE', 111),\n",
      " ('FACEBOOK', 109),\n",
      " ('NOTE', 105),\n",
      " ('MATE', 105),\n",
      " ('XL', 96),\n",
      " ('C', 94),\n",
      " ('BLUETOOTH', 84),\n",
      " ('NEW', 82),\n",
      " ('AUSTRALIA', 81),\n",
      " ('SOUTH', 81),\n",
      " ('XR', 78),\n",
      " ('LONDON', 77),\n",
      " ('AI', 72),\n",
      " ('KOREA', 71),\n",
      " ('RAM', 69),\n",
      " ('X', 69),\n",
      " ('TRUMP', 68),\n",
      " ('OLED', 61),\n",
      " ('P30', 61),\n",
      " ('MICROSOFT', 58),\n",
      " ('BLACK', 55),\n",
      " ('NETFLIX', 54),\n",
      " ('BUY', 53),\n",
      " ('AIRPODS', 52),\n",
      " ('SIRI', 49),\n",
      " ('FACE', 48),\n",
      " ('CHINESE', 48),\n",
      " ('SPOTIFY', 47),\n",
      " ('NOVEMBER', 47),\n",
      " ('WHERE', 47),\n",
      " ('SURFACE', 47)]\n"
     ]
    }
   ],
   "source": [
    "word_count = 0\n",
    "word_total_count = 0\n",
    "NNP_words = []\n",
    "for articles in content:\n",
    "    word_total_count += len(articles)\n",
    "    for words in articles:\n",
    "        if words[1] == 'NNP':\n",
    "            NNP_words.append(words[0].upper())\n",
    "            word_count += 1\n",
    "print(word_count/word_total_count * 100)\n",
    "NNP_Freq = FreqDist(NNP_words).most_common(50)\n",
    "pprint(NNP_Freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NEW', 538),\n",
      " ('GOOD', 410),\n",
      " ('LAST', 347),\n",
      " ('TOP', 270),\n",
      " ('SMALL', 207),\n",
      " ('AVAILABLE', 192),\n",
      " ('GREAT', 192),\n",
      " ('BIG', 181),\n",
      " ('SCREEN', 181),\n",
      " ('MUCH', 161),\n",
      " ('MANY', 155),\n",
      " ('FIRST', 141),\n",
      " ('FULL', 139),\n",
      " ('HIGH', 135),\n",
      " ('LITTLE', 133),\n",
      " ('SECOND', 120),\n",
      " ('LARGE', 117),\n",
      " ('THIRD', 112),\n",
      " ('RIGHT', 112),\n",
      " ('SIMILAR', 111),\n",
      " ('WIDE', 106),\n",
      " ('LIGHT', 101),\n",
      " ('NEXT', 101),\n",
      " ('BLACK', 100),\n",
      " ('UPDATE', 97),\n",
      " ('EXCELLENT', 95),\n",
      " ('MAIN', 93),\n",
      " ('OLD', 87),\n",
      " ('OPEN', 86),\n",
      " ('ABLE', 82),\n",
      " ('HARD', 81),\n",
      " ('CHINESE', 81),\n",
      " ('LOW', 80),\n",
      " ('DIFFERENT', 79),\n",
      " ('FREE', 78),\n",
      " ('LONG', 78),\n",
      " ('COMMERCIAL', 74),\n",
      " ('REAL', 74),\n",
      " ('TRADITIONAL', 73),\n",
      " ('RIVAL', 73),\n",
      " ('MEAN', 72),\n",
      " ('CLICK', 70),\n",
      " ('OPTICAL', 69),\n",
      " ('LIVE', 69),\n",
      " ('INDEPENDENT', 68),\n",
      " ('IMPORTANT', 65),\n",
      " ('TRUE', 64),\n",
      " ('PUBLIC', 64),\n",
      " ('SMART', 63),\n",
      " ('DIFFICULT', 63)]\n"
     ]
    }
   ],
   "source": [
    "feeling_words = []\n",
    "for articles in content:\n",
    "    word_total_count += len(articles)\n",
    "    for words in articles:\n",
    "        if words[1] == 'JJ':\n",
    "            feeling_words.append((words[0].upper()))\n",
    "\n",
    "Feeling_Freq = FreqDist(feeling_words).most_common(50)\n",
    "pprint(Feeling_Freq)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(content[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
