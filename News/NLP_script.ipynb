{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from pprint import pprint\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import FreqDist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading of .csv file\n",
    "df = pd.read_csv(\"TheGuardian.csv\")\n",
    "content = df['Text']\n",
    "assert content.isnull().count() == len(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before stop word removal count\n",
    "count = [len(content[i]) for i in range(len(content))]\n",
    "# data['Count Before'] = count\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "data = []\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "content = content.apply(lambda x: tokenizer.tokenize(x))\n",
    "content.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removal of Stop Words\n",
    "from nltk.corpus import stopwords\n",
    "set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = [w for w in text if w not in stopwords.words('english')]\n",
    "    return words\n",
    "\n",
    "content = content.apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization (OLD VERSION)\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# def word_lemmatizer(text):`\n",
    "#     lem_text = [lemmatizer.lemmatize(i, pos=\"v\") for i in text]\n",
    "#     return lem_text\n",
    "\n",
    "# content = content.apply(lambda x: word_lemmatizer(x))\n",
    "\n",
    "# content_freq = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def word_lemmatizer(text):\n",
    "    lem_text = [lemmatizer.lemmatize(i, pos=\"a\") for i in text]\n",
    "    lem_text = [lemmatizer.lemmatize(i, pos=\"s\") for i in text]\n",
    "    lem_text = [lemmatizer.lemmatize(i, pos=\"v\") for i in text]\n",
    "    lem_text = [lemmatizer.lemmatize(i, pos=\"r\") for i in text]\n",
    "    lem_text = [lemmatizer.lemmatize(i, pos=\"n\") for i in text]\n",
    "    return lem_text\n",
    "\n",
    "content = content.apply(lambda x: word_lemmatizer(x))\n",
    "print(content[0])\n",
    "content_freq = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count after removal of stop words\n",
    "countaft = [len(content[i]) for i in range(len(content))]\n",
    "print(countaft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tagging\n",
    "content_tag = [nltk.pos_tag(i) for i in content]\n",
    "print(content_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Difference for the count before and after stop word removal\n",
    "count_diff = []\n",
    "for i in range(len(count)):\n",
    "    count_diff.append(round(((count[i] - countaft[i])/count[i])*100, 1))\n",
    "print(count_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freq = [FreqDist(content_freq[i]).most_common(5) for i in range(len(content_freq))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Amount Cleansed (%)'] = count_diff\n",
    "# df['Frequency'] = Freq\n",
    "\n",
    "# df.to_csv('Output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_count = 0\n",
    "# word_total_count = 0\n",
    "# NNP_words = []\n",
    "# for articles in content:\n",
    "#     word_total_count += len(articles)\n",
    "#     for words in articles:\n",
    "#         if words[1] == 'NNP':\n",
    "#             NNP_words.append(words[0].upper())\n",
    "#             word_count += 1\n",
    "# print(word_count/word_total_count * 100)\n",
    "# NNP_Freq = FreqDist(NNP_words).most_common(50)\n",
    "# pprint(NNP_Freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feeling_words = []\n",
    "# for articles in content:\n",
    "#     word_total_count += len(articles)\n",
    "#     for words in articles:\n",
    "#         if words[1] == 'JJ':\n",
    "#             feeling_words.append((words[0].upper()))\n",
    "\n",
    "# Feeling_Freq = FreqDist(feeling_words).most_common(50)\n",
    "# pprint(Feeling_Freq)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Creating a main list of the the total articles with feeling words that contain the feeling words for the respective articles\n",
    "total_fw = []\n",
    "\n",
    "for articles in content_tag:\n",
    "    article_fw = []\n",
    "    for words in articles:\n",
    "        if words[1].startswith('JJ'):\n",
    "            article_fw.append(words)\n",
    "    total_fw.append(article_fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mapping of the tags to match the WordNet's POS \n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    \"\"\"\n",
    "    Convert between the PennTreebank tags to simple Wordnet tags\n",
    "    \"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "for article in total_fw:\n",
    "    for words in article:\n",
    "        penn_to_wn(words[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To get the sentiments based on the dictionary from WordNet that reads the phrases/sentence and returns a numerical value for\n",
    "#the +ve and -ve sentiments\n",
    "def get_sentiment(word,tag):\n",
    "    \"\"\" returns list of pos neg and objective score. But returns empty list if not present in senti wordnet. \"\"\"\n",
    "\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
    "        return []\n",
    "\n",
    "    lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "    if not lemma:\n",
    "        return []\n",
    "\n",
    "    synsets = wn.synsets(word, pos=wn_tag)\n",
    "    if not synsets:\n",
    "        return []\n",
    "\n",
    "    # Take the first sense, the most common\n",
    "    synset = synsets[0]\n",
    "    swn_synset = swn.senti_synset(synset.name())\n",
    "\n",
    "    return [swn_synset.pos_score(),swn_synset.neg_score(),swn_synset.obj_score()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The getting of sentiments for all the words in each article\n",
    "net_senti = []\n",
    "for article in total_fw:\n",
    "    article_senti = []\n",
    "    for words in article:\n",
    "        \n",
    "        senti = get_sentiment(words[0], words[1])\n",
    "        if len(senti) > 0:\n",
    "            print([words[0], (senti[0] - senti[1])])\n",
    "            article_senti.append([words[0], (senti[0] - senti[1])])\n",
    "    net_senti.append(article_senti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final pritting and to count the number of articles that are positive, negative or neutral based on the sentiment values\n",
    "article_senti = []\n",
    "for article in net_senti:\n",
    "    sentiment = 0\n",
    "    article_word_count = 0\n",
    "    for senti in article:\n",
    "        print(senti[0], senti[1])\n",
    "        sentiment += senti[1]\n",
    "        article_word_count += 1\n",
    "    print(\"\\n********************************************\\nTotal Sentiment: \", sentiment)\n",
    "    print(\"Word Count In Article: \", article_word_count, \"\\n********************************************\")\n",
    "    print(\"\\n**********************\\n NEXT ARTICLE \\n**********************\")\n",
    "    article_senti.append(sentiment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
